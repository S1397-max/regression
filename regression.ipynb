{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwetU-GaSWyU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and an independent variable (X) using a straight-line equation:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        "where\n",
        "𝑚\n",
        " is the slope and\n",
        "𝑐\n",
        " is the intercept.\n",
        "\n",
        "2. Linearity: The relationship between X and Y is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: The variance of residuals is constant across all levels of X.\n",
        "\n",
        "Normality: Residuals (errors) are normally distributed.\n",
        "\n",
        "3. The coefficient\n",
        "𝑚\n",
        " represents the slope of the regression line, indicating the change in Y for a one-unit increase in X.\n",
        "\n",
        "4. The intercept\n",
        "𝑐\n",
        " represents the predicted value of Y when X is zero. It essentially defines where the regression line crosses the Y-axis.\n",
        "\n",
        "5. The slope\n",
        "𝑚\n",
        " is calculated using the formula:\n",
        "\n",
        "𝑚\n",
        "=\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "(\n",
        "𝑌\n",
        "𝑖\n",
        "−\n",
        "𝑌\n",
        "ˉ\n",
        ")\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "where\n",
        "𝑋\n",
        "𝑖\n",
        ",\n",
        "𝑌\n",
        "𝑖\n",
        " are the individual data points and\n",
        "𝑋\n",
        "ˉ\n",
        ",\n",
        "𝑌\n",
        "ˉ\n",
        " are the mean values of X and Y.\n",
        "\n",
        "6. The least squares method minimizes the sum of squared residuals (differences between actual and predicted values) to find the best-fitting regression line.\n",
        "\n",
        "7. R² measures the proportion of variance in Y explained by X. It ranges from 0 to 1:\n",
        "\n",
        "R² = 1: Perfect fit, X explains all variation in Y.\n",
        "\n",
        "R² = 0: No relationship between X and Y.\n",
        "\n",
        "8. Multiple Linear Regression extends Simple Linear Regression to model relationships between multiple independent variables and a dependent variable:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝑚\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑚\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝑐\n",
        "9. Simple Linear Regression: Involves only one independent variable.\n",
        "\n",
        "Multiple Linear Regression: Involves two or more independent variables.\n",
        "\n",
        "10. Same assumptions as Simple Linear Regression (linearity, independence, homoscedasticity, normality).\n",
        "\n",
        "No multicollinearity: Independent variables should not be highly correlated.\n",
        "\n",
        "No autocorrelation: Errors should not be correlated.\n",
        "\n",
        "11. Heteroscedasticity occurs when the variance of residuals is not constant across different values of the independent variables. This can lead to inefficient estimates, invalid significance tests, and unreliable confidence intervals.\n",
        "\n",
        "12. Remove highly correlated predictors if they don’t add value.\n",
        "\n",
        "Use Principal Component Analysis (PCA) to reduce dimensionality.\n",
        "\n",
        "Standardize or normalize variables.\n",
        "\n",
        "Use Ridge or Lasso regression, which are designed for multicollinearity.\n",
        "\n",
        "13. One-hot encoding: Converts categorical values into binary variables.\n",
        "\n",
        "Label encoding: Assigns numeric values to categories.\n",
        "\n",
        "Dummy variables: Creates separate variables for each category.\n",
        "\n",
        "Target encoding: Maps categories based on the mean of the target variable.\n",
        "\n",
        "14. Interaction terms help capture the combined effect of two independent variables on the dependent variable. They are useful when the impact of one predictor depends on the value of another.\n",
        "\n",
        "15. Simple Linear Regression: The intercept represents the expected value of Y when X is zero.\n",
        "\n",
        "Multiple Linear Regression: The intercept represents the expected value of Y when all independent variables are zero, which may not always be meaningful.\n",
        "\n",
        "16. The slope indicates the rate of change in Y for a one-unit change in the predictor variable. A larger absolute slope means a stronger relationship between X and Y.\n",
        "\n",
        "17. Does not indicate if predictors are meaningful.\n",
        "\n",
        "Does not account for overfitting—a high R² doesn’t mean good generalization.\n",
        "\n",
        "Cannot measure whether the model is the best among alternatives.\n",
        "\n",
        "Adjusted R² is preferred when comparing models with different numbers of predictors.\n",
        "\n",
        "18. A large standard error suggests high variability in coefficient estimates, meaning the predictor variable may not be very reliable in explaining Y.\n",
        "\n",
        "19. Polynomial regression is a variation of linear regression where the relationship between the independent and dependent variables is modeled as a polynomial function, rather than a straight line.\n",
        "\n",
        "20. Polynomial regression is used when the relationship between X and Y is nonlinear, and a curved trend fits the data better than a straight line.\n",
        "\n",
        "21. The intercept represents the predicted value of the dependent variable when all independent variables are zero. It helps establish the starting point of the regression equation and can sometimes have a meaningful interpretation, depending on the context.\n",
        "\n",
        "22. Heteroscedasticity can be identified by plotting residuals against fitted values. If the spread of residuals increases or forms a cone-shaped pattern, it suggests heteroscedasticity. Addressing it is important because it affects the reliability of standard errors, leading to invalid statistical conclusions.\n",
        "\n",
        "23. A high\n",
        "𝑅\n",
        "2\n",
        " but low adjusted\n",
        "𝑅\n",
        "2\n",
        " suggests that the model is overfitting, meaning it includes too many predictors that don’t significantly contribute to explaining the dependent variable. Adjusted\n",
        "𝑅\n",
        "2\n",
        " penalizes excessive predictors and helps assess the true predictive power of the model.\n",
        "\n",
        "24. Scaling ensures that all variables have comparable units and prevents certain predictors from dominating the model simply due to larger numerical values. Standardization (subtracting the mean and dividing by the standard deviation) or normalization (rescaling between 0 and 1) improves numerical stability and interpretation.\n",
        "\n",
        "25. Linear regression assumes a straight-line relationship between X and Y.\n",
        "\n",
        "Polynomial regression fits a curved relationship by including higher-degree terms such as\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        ", etc.\n",
        "\n",
        "26. The general equation for a degree\n",
        "𝑛\n",
        " polynomial regression model is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "0\n",
        "+\n",
        "𝑚\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑚\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑚\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝑐\n",
        "where\n",
        "𝑛\n",
        " is the degree of the polynomial.\n",
        "\n",
        "27. Yes! Polynomial regression can be extended to multiple variables by including polynomial terms for each predictor. Example:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "0\n",
        "+\n",
        "𝑚\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝑚\n",
        "2\n",
        "𝑋\n",
        "1\n",
        "2\n",
        "+\n",
        "𝑚\n",
        "3\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝑚\n",
        "4\n",
        "𝑋\n",
        "2\n",
        "2\n",
        "+\n",
        "𝑐\n",
        "However, too many polynomial terms can lead to overfitting, so careful selection is necessary.\n",
        "\n",
        "28. Overfitting: Higher-degree polynomials can fit training data too well but generalize poorly.\n",
        "\n",
        "Complex interpretation: Coefficients become harder to interpret.\n",
        "\n",
        "Extrapolation issues: Predictions outside the observed range can be highly unreliable.\n",
        "\n",
        "Numerical instability: Large polynomial degrees can lead to computational instability.\n",
        "\n",
        "29. Cross-validation: Splitting data into training and validation sets to check performance.\n",
        "\n",
        "Adjusted\n",
        "𝑅\n",
        "2\n",
        ": Helps prevent unnecessary complexity.\n",
        "\n",
        "Mean Squared Error (MSE): Measures prediction error.\n",
        "\n",
        "Residual plots: Checks if errors follow random patterns.\n",
        "\n",
        "Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC): Help balance model complexity and accuracy.\n",
        "\n",
        "30. Visualization helps:\n",
        "\n",
        "Assess the fit of the regression curve.\n",
        "\n",
        "Identify overfitting by seeing if the curve follows excessive fluctuations.\n",
        "\n",
        "Verify trends in data.\n",
        "\n",
        "Choose the appropriate polynomial degree before applying statistical techniques.\n",
        "\n",
        "31. Polynomial regression can be implemented using Python libraries such as NumPy, Scikit-learn, and Matplotlib. A typical approach includes:\n",
        "\n",
        "Generating polynomial features using PolynomialFeatures from Scikit-learn.\n",
        "\n",
        "Fitting the model using LinearRegression().\n",
        "\n",
        "Visualizing the results using Matplotlib."
      ],
      "metadata": {
        "id": "FZnh4jovSlBY"
      }
    }
  ]
}